import os
import random
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt

# -------------------------------
# New auth function using CNN coding
# -------------------------------
def auth(training_file_paths, testing_file_paths, used_device, TEST_N):
    
    # ============================
    # Data Loading Function
    # ============================
    def load_dataset(file_path, target_user):
        """
        Loads a text file with columns: time, x, y, z.
        Returns a TensorDataset where the input data is reshaped to (N,1,3)
        and the target is 1 (if target_user is True) or 0.
        """
        df = pd.read_csv(file_path, header=None, names=['time', 'x', 'y', 'z'])
        # Get the x, y, z values and reshape to (N, 1, 3) for CNN input.
        data = torch.tensor(df[['x', 'y', 'z']].values, dtype=torch.float32)
        data = data.unsqueeze(1)  # now shape is (N, 1, 3)
        targets = torch.ones((len(data), 1), dtype=torch.float32) if target_user else torch.zeros((len(data), 1), dtype=torch.float32)
        return TensorDataset(data, targets), df

    # ============================
    # CNN Model Definition
    # ============================
    class CNNClassifier(nn.Module):
        def __init__(self, input_channels, conv_channels, kernel_size, pool_size, num_classes):
            super(CNNClassifier, self).__init__()
            self.conv1 = nn.Conv1d(in_channels=input_channels, out_channels=conv_channels, kernel_size=kernel_size)
            self.relu = nn.ReLU()
            self.pool = nn.MaxPool1d(kernel_size=pool_size)
            # Calculate the size after conv and pooling. For input length=3:
            # After conv1d: length = (3 - kernel_size + 1)
            # After maxpool1d: length = floor(conv_length/pool_size)
            self.fc = nn.Linear(conv_channels * 1, num_classes)
            self.sigmoid = nn.Sigmoid()

        def forward(self, x):
            # x shape: (batch_size, 1, 3)
            x = self.conv1(x)
            x = self.relu(x)
            x = self.pool(x)
            x = x.view(x.size(0), -1)  # flatten
            x = self.fc(x)
            x = self.sigmoid(x)
            return x

    # ============================
    # Hyperparameter Inputs
    # ============================
    BATCH_SIZE = 64
    EPOCHS = 8
    LEARNING_RATE = 0.001
    INPUT_SIZE = 3        # number of features (x, y, z)
    NUM_CLASSES = 1       # binary classification (sigmoid output)

    # CNN-specific hyperparameters
    INPUT_CHANNELS = 1    # input reshaped to (N, 1, 3)
    CONV_CHANNELS = 2     # number of filters in the conv layer
    KERNEL_SIZE = 2       # kernel size for conv1d
    POOL_SIZE = 2         # pooling size

    print("Training and validating for", used_device, "using CNN method...")
    u = 0  # index for labeling

    # List to store ROC data for each user (for combined ROC plotting)
    all_users_roc_data = []
        
    # -------------------------------
    # Training loop: one model per training file
    # -------------------------------
    for u in range(len(training_file_paths)):
        print(f"\n=== Processing Authentication for User {u+1} ===")
        # Train on each training file: positive if file index equals current user; negative otherwise.
        for i, file_path in enumerate(training_file_paths):
            dataset, _ = load_dataset(file_path, target_user=(i == u))
            data_loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)
            
            # Initialize the CNN model using hyperparameters above.
            model = CNNClassifier(INPUT_CHANNELS, CONV_CHANNELS, KERNEL_SIZE, POOL_SIZE, NUM_CLASSES)
            criterion = nn.BCELoss()
            optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
            
            # Training Loop
            for epoch in range(EPOCHS):
                total_loss = 0.0
                for inputs, targets in data_loader:
                    outputs = model(inputs)
                    loss = criterion(outputs, targets)
                    optimizer.zero_grad()
                    loss.backward()
                    optimizer.step()
                    total_loss += loss.item()
                print(f"Dataset {i + 1}, Epoch {epoch + 1}/{EPOCHS}, Loss: {total_loss / len(data_loader):.4f}")
            
            model_save_path = f"D:/Spring semester 2025/MLS/Models/cnn_model_user_{i + 1}.pth"
            torch.save(model.state_dict(), model_save_path)
        
        # -------------------------------
        # Testing loop: evaluate models on testing files with repeated sampling and aggregate metrics
        # -------------------------------
        num_iterations = 20
        metrics_list = []
        roc_data = {}  # To store ROC data for this user from the final iteration
        
        for iteration in range(num_iterations):
            all_outputs = []
            all_targets = []
            for i, file_path in enumerate(testing_file_paths):
                # Load the testing dataset (with balanced target sampling)
                dataset, _ = load_dataset(file_path, target_user=(i == u))
                sample_size = TEST_N if (i == u) else int(TEST_N / 16)
                sample_indices = random.sample(range(len(dataset)), min(sample_size, len(dataset)))
                sampled_data = torch.stack([dataset[idx][0] for idx in sample_indices])
                sampled_targets = torch.stack([dataset[idx][1] for idx in sample_indices])
                
                # Load the corresponding saved model
                model = CNNClassifier(INPUT_CHANNELS, CONV_CHANNELS, KERNEL_SIZE, POOL_SIZE, NUM_CLASSES)
                model_load_path = f"D:/Spring semester 2025/MLS/Models/cnn_model_user_{i + 1}.pth"
                state_dict = torch.load(model_load_path)
                model.load_state_dict(state_dict)
                model.eval()
                
                with torch.no_grad():
                    outputs = model(sampled_data)
                    all_outputs.extend(outputs.cpu().numpy().flatten().tolist())
                    all_targets.extend(sampled_targets.cpu().numpy().flatten().tolist())
            
            # Convert lists to arrays
            outputs_arr = np.array(all_outputs)
            targets_arr = np.array(all_targets)
            predictions = (outputs_arr >= 0.5).astype(int)
            
            tp = ((predictions == 1) & (targets_arr == 1)).sum()
            tn = ((predictions == 0) & (targets_arr == 0)).sum()
            fp = ((predictions == 1) & (targets_arr == 0)).sum()
            fn = ((predictions == 0) & (targets_arr == 1)).sum()
            
            # Compute performance metrics
            fnr = 100 * fn / (fn + tp) if (fn + tp) > 0 else 0
            fpr = 100 * fp / (fp + tn) if (fp + tn) > 0 else 0
            tpr = 100 * tp / (tp + fn) if (tp + fn) > 0 else 0
            tnr = 100 * tn / (tn + fp) if (tn + fp) > 0 else 0
            accuracy = 100 * (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0
            f1_score = 100 * (2 * tp) / (2 * tp + fp + fn) if (2 * tp + fp + fn) > 0 else 0
            
            try:
                aur = roc_auc_score(targets_arr, outputs_arr) * 100
            except Exception:
                aur = 0.0
            
            try:
                fpr_vals, tpr_vals, _ = roc_curve(targets_arr, outputs_arr)
                fnr_vals = 1 - tpr_vals
                abs_diffs = np.abs(fpr_vals - fnr_vals)
                min_index = np.argmin(abs_diffs)
                eer = ((fpr_vals[min_index] + fnr_vals[min_index]) / 2) * 100
            except Exception:
                eer = 0.0
            
            metrics_list.append([tp, tn, fp, fn, fnr, fpr, tpr, tnr, accuracy, f1_score, aur, eer])
            
            # Save ROC data from the final iteration
            if iteration == num_iterations - 1:
                roc_data = {'fpr': fpr_vals, 'tpr': tpr_vals, 'auc': aur}
        
        # Store ROC data for this user
        if roc_data:
            all_users_roc_data.append((u+1, roc_data['fpr'], roc_data['tpr'], roc_data['auc']))
        
        # Compute average and standard deviation of metrics
        metrics_array = np.array(metrics_list)
        avg_metrics = np.mean(metrics_array, axis=0)
        std_metrics = np.std(metrics_array, axis=0)
        
        metric_names = ["TP", "TN", "FP", "FN", "FNR", "FPR", "TPR", "TNR", "Accuracy", "F1-score", "AUR", "EER%"]
        
        # Create DataFrame for iteration metrics and summary statistics
        iterations = list(range(1, num_iterations + 1))
        iteration_results = pd.DataFrame(metrics_list, columns=metric_names)
        iteration_results.insert(0, "Iteration", iterations)
        summary_results = pd.DataFrame([
            ["Average"] + list(avg_metrics),
            ["Std Dev"] + list(std_metrics)
        ], columns=["Metric"] + metric_names)
        
        # Write results to Excel (one file per user)
        excel_path = f"D:/Spring semester 2025/MLS/Results/CNN/confusion_table_CNN_{used_device}{u+1}.xlsx"
        with pd.ExcelWriter(excel_path) as writer:
            iteration_results.to_excel(writer, sheet_name="Iteration Metrics", index=False)
            summary_results.to_excel(writer, sheet_name="Summary", index=False)
        print(f"Confusion table with iteration metrics and summary saved to {excel_path}")

    # -------------------------------
    # After processing all users, plot combined ROC curves
    # -------------------------------
    if all_users_roc_data:
        plt.figure(figsize=(10, 8))
        for user_id, fpr, tpr, auc in all_users_roc_data:
            plt.plot(fpr, tpr, label=f'User {user_id} (AUC = {auc:.2f})')
        plt.plot([0, 1], [0, 1], 'k--')
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('ROC Curves for All Users (CNN)')
        # Place legend outside the plot (topâ€“right)
        plt.legend(loc='upper left', bbox_to_anchor=(1.05, 1), borderaxespad=0.)
        
        # Use the folder of the last Excel output for saving the ROC plot
        output_folder = os.path.dirname(excel_path)
        roc_jpg_path = os.path.join(output_folder, f'roc_curves__CNN_{used_device}.jpg')
        plt.savefig(roc_jpg_path, format='jpg', bbox_inches='tight')
        print(f"Combined ROC curves saved as JPG at {roc_jpg_path}")
        #plt.show()
    
    # -------------------------------
    # Generate Excel file with ROC curve data for each user
    # -------------------------------
    if all_users_roc_data:
        roc_excel_path = os.path.join(output_folder, f'roc_curves_CNN_{used_device}.xlsx')
        with pd.ExcelWriter(roc_excel_path) as writer:
            auc_summary = []
            for user_id, fpr, tpr, auc in all_users_roc_data:
                df_roc = pd.DataFrame({'FPR': fpr, 'TPR': tpr})
                df_roc.to_excel(writer, sheet_name=f'User {user_id}', index=False)
                auc_summary.append({'User': user_id, 'AUC': auc})
            summary_df_roc = pd.DataFrame(auc_summary)
            summary_df_roc.to_excel(writer, sheet_name='AUC Summary', index=False)
        print(f"ROC curves data saved as Excel at {roc_excel_path}")

# --------------------------------------------------
# File Paths for Different Devices and Running the Authentication
# --------------------------------------------------

# For Samsung
used_device = 'SAM'
TEST_N = 5000   # Number of samples for target user in testing; others get TEST_N/16
training_file_paths = [
    "D:/Spring semester 2025/MLS/First Data set/data/Training Data/User001/Samsung - back/accelDataM.txt",
    "D:/Spring semester 2025/MLS/First Data set/data/Training Data/User002/Samsung - back/accelDataM.txt",
    "D:/Spring semester 2025/MLS/First Data set/data/Training Data/User003/Samsung - back/accelDataM.txt",
    "D:/Spring semester 2025/MLS/First Data set/data/Training Data/User004/Samsung - back/accelDataM.txt",
    "D:/Spring semester 2025/MLS/First Data set/data/Training Data/User005/Samsung - back/accelDataM.txt",
    "D:/Spring semester 2025/MLS/First Data set/data/Training Data/User006/Samsung - back/accelDataM.txt",
    "D:/Spring semester 2025/MLS/First Data set/data/Training Data/User007/Samsung - back/accelDataM.txt",
    "D:/Spring semester 2025/MLS/First Data set/data/Training Data/User008/Samsung - back/accelDataM.txt",
    "D:/Spring semester 2025/MLS/First Data set/data/Training Data/User009/Samsung - back/accelDataM.txt",
    "D:/Spring semester 2025/MLS/First Data set/data/Training Data/User0010/Samsung - back/accelDataM.txt",
    "D:/Spring semester 2025/MLS/First Data set/data/Training Data/User0011/Samsung - back/accelDataM.txt",
    "D:/Spring semester 2025/MLS/First Data set/data/Training Data/User0012/Samsung - back/accelDataM.txt",
    "D:/Spring semester 2025/MLS/First Data set/data/Training Data/User0013/Samsung - back/accelDataM.txt",
    "D:/Spring semester 2025/MLS/First Data set/data/Training Data/User0014/Samsung - back/accelDataM.txt",
    "D:/Spring semester 2025/MLS/First Data set/data/Training Data/User0015/Samsung - back/accelDataM.txt",
    "D:/Spring semester 2025/MLS/First Data set/data/Training Data/User0016/Samsung - back/accelDataM.txt",
    "D:/Spring semester 2025/MLS/First Data set/data/Training Data/User0017/Samsung - back/accelDataM.txt"
]
testing_file_paths = [p.replace("Training Data", "Testing Data") for p in training_file_paths]
auth(training_file_paths, testing_file_paths, used_device, TEST_N)

# For HTC
used_device = 'HTC'
TEST_N = 5000
training_file_paths = [
    "D:/Spring semester 2025/MLS/First Data set/data/Training Data/User001/HTC - front/accelDataM.txt",
    "D:/Spring semester 2025/MLS/First Data set/data/Training Data/User002/HTC - front/accelDataM.txt",
    "D:/Spring semester 2025/MLS/First Data set/data/Training Data/User003/HTC - front/accelDataM.txt",
    "D:/Spring semester 2025/MLS/First Data set/data/Training Data/User004/HTC - front/accelDataM.txt",
    "D:/Spring semester 2025/MLS/First Data set/data/Training Data/User005/HTC - front/accelDataM.txt",
    "D:/Spring semester 2025/MLS/First Data set/data/Training Data/User006/HTC - front/accelDataM.txt",
    "D:/Spring semester 2025/MLS/First Data set/data/Training Data/User007/HTC - front/accelDataM.txt",
    "D:/Spring semester 2025/MLS/First Data set/data/Training Data/User008/HTC - front/accelDataM.txt",
    "D:/Spring semester 2025/MLS/First Data set/data/Training Data/User009/HTC - front/accelDataM.txt",
    "D:/Spring semester 2025/MLS/First Data set/data/Training Data/User0010/HTC - front/accelDataM.txt",
    "D:/Spring semester 2025/MLS/First Data set/data/Training Data/User0011/HTC - front/accelDataM.txt",
    "D:/Spring semester 2025/MLS/First Data set/data/Training Data/User0012/HTC - front/accelDataM.txt",
    "D:/Spring semester 2025/MLS/First Data set/data/Training Data/User0013/HTC - front/accelDataM.txt",
    "D:/Spring semester 2025/MLS/First Data set/data/Training Data/User0014/HTC - front/accelDataM.txt",
    "D:/Spring semester 2025/MLS/First Data set/data/Training Data/User0015/HTC - front/accelDataM.txt",
    "D:/Spring semester 2025/MLS/First Data set/data/Training Data/User0016/HTC - front/accelDataM.txt",
    "D:/Spring semester 2025/MLS/First Data set/data/Training Data/User0017/HTC - front/accelDataM.txt"
]
testing_file_paths = [p.replace("Training Data", "Testing Data") for p in training_file_paths]
auth(training_file_paths, testing_file_paths, used_device, TEST_N)

# For Google Glasses
used_device = 'GOO'
TEST_N = 704   # Adjusted sample size for target user; others get TEST_N/16
training_file_paths = [
    "D:/Spring semester 2025/MLS/First Data set/data/Training Data/User001/Glass/accelData.txt",
    "D:/Spring semester 2025/MLS/First Data set/data/Training Data/User002/Glass/accelData.txt",
    "D:/Spring semester 2025/MLS/First Data set/data/Training Data/User003/Glass/accelData.txt",
    "D:/Spring semester 2025/MLS/First Data set/data/Training Data/User004/Glass/accelData.txt",
    "D:/Spring semester 2025/MLS/First Data set/data/Training Data/User005/Glass/accelData.txt",
    "D:/Spring semester 2025/MLS/First Data set/data/Training Data/User006/Glass/accelData.txt",
    "D:/Spring semester 2025/MLS/First Data set/data/Training Data/User007/Glass/accelData.txt",
    "D:/Spring semester 2025/MLS/First Data set/data/Training Data/User008/Glass/accelData.txt",
    "D:/Spring semester 2025/MLS/First Data set/data/Training Data/User009/Glass/accelData.txt",
    "D:/Spring semester 2025/MLS/First Data set/data/Training Data/User0010/Glass/accelData.txt",
    "D:/Spring semester 2025/MLS/First Data set/data/Training Data/User0011/Glass/accelData.txt",
    "D:/Spring semester 2025/MLS/First Data set/data/Training Data/User0012/Glass/accelData.txt",
    "D:/Spring semester 2025/MLS/First Data set/data/Training Data/User0013/Glass/accelData.txt",
    "D:/Spring semester 2025/MLS/First Data set/data/Training Data/User0014/Glass/accelData.txt",
    "D:/Spring semester 2025/MLS/First Data set/data/Training Data/User0015/Glass/accelData.txt",
    "D:/Spring semester 2025/MLS/First Data set/data/Training Data/User0016/Glass/accelData.txt",
    "D:/Spring semester 2025/MLS/First Data set/data/Training Data/User0017/Glass/accelData.txt"
]
testing_file_paths = [p.replace("Training Data", "Testing Data") for p in training_file_paths]
auth(training_file_paths, testing_file_paths, used_device, TEST_N)
